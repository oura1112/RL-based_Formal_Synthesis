This letter adopts a reinforcement learning-based approach to learn a
control policy where the control objective is specified as a linear
temporal logic specification. The system is modeled by a Markov
decision process and the LTL specification is transformed to a
limit-deterministic Buchi automaton (LDBA). The novelty of the proposed
approach is the introduction of an augmented LDBA that keeps track of
previous visits to the accepting sets. The effectiveness of the
proposed method is illustrated through an example involving robot path
planning. My comments are given below:

i) The paper is well-written and organized well. There are lot of
symbols with subscripts, superscripts in the paper and it might be
helpful to include a paragraph that describes the notation. 

notationを説明する段落を追記．


ii) How is the reward function defined in the example for the method in
[14]? Can the two rewards be compared? 

同じ報酬関数を用いている．



iii) How does the proposed method compare in terms of computational
complexity with other methods? This would be helpful information for
the readers especially since the proposed method depends on augmenting
the LDBA.

ほかの手法と比べて，計算複雑性がどの程度かを述べる．
