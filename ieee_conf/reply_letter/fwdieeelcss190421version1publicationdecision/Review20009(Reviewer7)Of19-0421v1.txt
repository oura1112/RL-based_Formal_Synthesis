
In this paper, the author presented a new reinforcement learning
algorithm to optimally planning in MDP given linear temporal logic
formulas. They use limit deterministic buchi automaton and presented
the algorithm using the product between the MDP and an augmented
automaton, constructed from the limit deterministic one. 
The paper studies an important problem in probabilistic model checking
and has unique approaches to that problem. the reviewer has the
following technical questions: 

- The probability function in Def. 9 can be not well-defined for
non-deterministic transitions. Consider a simple example, say (x1, l,
x1・) and (x1, l, x2・ are both in delta. P(s竹s, a) =1 with l=
L((s,a,s・) will have two outgoing transitions labeled by the same
action a, and all with probability one. The sum of probability given
action a on state (s, x1) will be 2. Will this study only consider
deterministic transitions?

tLDBAの定義にipsiron-transitionを追記．product MDPの定義にipsiron-transitionに対する遷移確率の定義を追記．
（ページ数超えそう）


- The construction of augmented automaton in Def. 8 is not
well-motivated. What is the intuition behind this construction? 
Further, there should be a formal proof that the two automata accepting
the same language. Examples in section IV helps a lot. It would be
useful to have a small example to illustrate the construction. 
- In the proof of Lemma 1, the last sentence is unclear. The goal is to
show that either an agent receives no reward from visiting recurrent
class, or the agent has to visit 殿ll recurrent class for which the
acceptance condition are satisfied・ It is not clear why it is not
possible that a policy only visits some subset of acceptance set but
not all. This may due to the lack of explanation to the def.8. 

通常のtLDBA(non-generalized)を用いる場合と比べて，遷移の総数に対する受理遷移の割合が格段に多くなるので，
報酬のスパース性が緩和されることを主張．またこの拡張により，元のMDP上にfinite-memolyのLTLを満たす方策が存在すれば，
product上にもLTLを満たすpositional policyが存在する．


- The construction of augmented automaton is similar to the 殿ccepting
frontier・in the following paper, presented at CDC this year: Hosein et
al.  mainly, the same problem was studied too. The authors should
highlight the difference and their unique contribution comparing to
existing work. 
Reinforcement Learning for Temporal Logic Control Synthesis with
Probabilistic Satisfaction Guarantees  
https://arxiv.org/pdf/1909.05304.pdf

accepting frontier functionはmemolylessなので，
元のMDP上にLTLを満たす方策が存在してもproduct上でそのLTLを満たすpositional policyが存在しないケースがある（our example)


- The comparison with [14] is unclear. If both methods generate the
optimal policies, then it is not clear why [14] does not perform
correctly in this example. 

同上．


Other nontechnical comments:

- The notations are too complicated and can be simplified. 
-Def. 7 is bit complicated in writing, the definition in ref.[13] is
much clearer. Further, the statement 鍍he transitions in each part are
deterministic・・this claim is not suggested in [13]. In fact, in the
original definition, the state space partitions to the nondeterministic
part and deterministic part. The transitions in the nondeterministic
part can be nondeterministic. 

LDBAのiniial partはnon-deterministicかも．

