\documentclass[10 pt, dvipdfmx]{article}

\usepackage{amsfonts,amsmath,amssymb,amsthm}
\usepackage{bm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{color}
%\usepackage[dvipdfmx]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{txfonts}
%\usepackage{ascmac, here}
\usepackage{listings}
\usepackage{color}
%\usepackage{url}
\usepackage{comment}

\allowdisplaybreaks[1]

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{prop}{Proposition}
%\newtheorem{definition}{Definition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}

\newcommand{\mysps}{\ensuremath{[\![s^{\otimes}]\!]}_s}
\newcommand{\myspq}{\ensuremath{[\![s^{\otimes}]\!]}_q}
\newcommand{\myspds}{\ensuremath{[\![s^{\otimes \prime}]\!]}_s}
\newcommand{\myspdq}{\ensuremath{[\![s^{\otimes \prime}]\!]}_q}
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}

\begin{document}
This reply is for the reviewer 5.

In the following, we refer to limit-diterministic generalized B\"{u}chi automaton as LDGBA and non-generalized one as LDBA.

\begin{enumerate}
  \item Contributions in our paper. \\
  Our contributions are as follows.
  \begin{itemize}
    \item The sparsity of rewards is relaxed comparing with using LDGBA by introducing the augmentation of LDBAs. Therefore, our proposed algorithm is more sample efficient.
    \item The recurrent classes of the Markov chain induced by a product MDP $M^{\otimes}$ and a positional policy $\pi$ on $M^{\otimes}$ are classified as ones that has at least one accepting transition in each accepting set or ones that has no accepting transition in all accepting sets without depending on the policy.
  \end{itemize}

  \item Discussions in the section with the simulation. \\
  If we construct product MDP $M^{\otimes}$ of an MDP and an LDGBA without any augmentation corresponding to a given LTL formula $\varphi$, a policy satisfying $\varphi$ may not be exist depending on $M^{\otimes}$ and $\varphi$. We show that an example in which there is not policies satisfying an LTL formula $\varphi$ when using the corresponding LDGBA without any augmentation.

  \item The motivations of using LDBA. \\
  the motivation of using LDGBA is to relax the sparsity of rewards. the motivation of our augmentation is to circulate all accepting sets without depending on an MDP, an LTL formula, and a policy.

  \item Comparing to the size of state space of the product MDP with the augmented LDGBA and one with the non-augmented LDGBA. \\


  \item Does maximizing the collection of the proposed rewards implies the maximization of the satisfaction probability? \\
  No, it does. It does not generally hold. However, our proposed method can be combined with some research results of attempting to maximize the satisfaction probability such as \cite{Hahn2019}.
\end{enumerate}

\begin{thebibliography}{99}
\bibitem{Hahn2019}
E.\ M.\ Hahn, M.\ Perez, S.\ Schewe, F.\ Somenzi, A.\ Triverdi, and D.\ Wojtczak,
``Omega-regular objective in model-free reinforcement learning,''
\textit{Lecture Notes in Computer Science}, no.\ 11427, pp.\ 395--412, 2019.
\end{thebibliography}

\end{document}
