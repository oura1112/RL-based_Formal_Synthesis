\relax 
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}First of All}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Next of First of All}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}First Subsection}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{First Subsubsection}{1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Test of Table Caption}}{1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Test of Figure Caption}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}2nd Chapter}{2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Processes and Reinforcement Learning}{2}\protected@file@percent }
\newlabel{MDP}{{2.1}{2}}
\citation{ESS}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Stochastic Discrete Event Systems}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Linear Temporal Logic and Automata}{4}\protected@file@percent }
\newlabel{def5}{{2.8}{5}}
\citation{SEJK2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}3rd Chapter}{7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Reinforcement-Learning-Based Synthesis of Control Policy}{7}\protected@file@percent }
\newlabel{lemma1}{{3.1}{8}}
\newlabel{theorem1}{{3.1}{9}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces RL-based synthesis of control policy on the MDP with the augmented tLDBA.}}{9}\protected@file@percent }
\newlabel{alg1}{{1}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Example}{9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The environment consisting of eight rooms and one corridor. Red arcs are the transitions that we want to occur infinitely often, while blue arcs are the transitions that we never want to occur. $s_7$ is the initial state.}}{10}\protected@file@percent }
\newlabel{Grid1}{{3.1}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The tLDBA recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $x_0$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to, e.g., \textcircled {\fontsize  {7\jsc@mpt }{8\jsc@mpt }\selectfont  \kanjiskip =0zw plus .1zw minus .01zw \xkanjiskip =0.25em plus 0.15em minus 0.06em 1}$a \land \neg b \land \neg c$ means the transition labeled by it belongs to the accepting set $F_1$.}}{10}\protected@file@percent }
\newlabel{automaton}{{3.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The augmented automaton for the tLDBA in Fig.\nobreakspace  {}3.2\@setref@ {} recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $(x_0, (0,0)^T )$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to. All states corresponding to $x_1$ are merged into $(x_1, (*,*)^T )$.}}{10}\protected@file@percent }
\newlabel{automaton_aug}{{3.3}{10}}
\citation{Owl}
\citation{Singh1998}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\citation{HAK2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The arithmetic mean of average reward in each episode for 20 learning sessions obtained from our proposed method (left) and the method by Hasanbeig $et\ al.$\cite  {HAK2019} (right). They are plotted per 100 episodes and the green areas represent the range of standard deviations.}}{12}\protected@file@percent }
\newlabel{result}{{3.4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The optimal policy obtained from our proposed method (left) and the method by Hasanbeig $et\ al.$\cite  {HAK2019} (right).}}{12}\protected@file@percent }
\newlabel{optimal}{{3.5}{12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}4th Chapter}{13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{ESS}
\citation{ESS}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs}{15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq15}{{A.1}{15}}
\citation{ESS}
\citation{ISP}
\citation{ESS}
\citation{SM}
\@writefile{toc}{\contentsline {chapter}{Acknowledgment}{18}\protected@file@percent }
\bibcite{BK2008}{1}
\bibcite{Clarke2018}{2}
\bibcite{KB2008}{3}
\bibcite{Gazit2009}{4}
\bibcite{WTM2012a}{5}
\bibcite{SU2018}{6}
\bibcite{Belta2017}{7}
\bibcite{Puterman}{8}
\bibcite{WTM2012}{9}
\bibcite{Sadigh2014}{10}
\bibcite{Sutton}{11}
\bibcite{HU2015}{12}
\bibcite{SEJK2016}{13}
\@writefile{toc}{\contentsline {chapter}{References}{19}\protected@file@percent }
\bibcite{HAK2019}{14}
\bibcite{Hahn2019}{15}
\bibcite{HKAKPL2019}{16}
\bibcite{BWZP2019}{17}
\bibcite{ESS}{18}
\bibcite{ISP}{19}
\bibcite{SM}{20}
\bibcite{Singh1998}{21}
\bibcite{Owl}{22}
