\relax 
\@writefile{toc}{\contentsline {chapter}{Abstract}{i}}
\citation{BK2008}
\citation{Clarke2018}
\citation{KB2008}
\citation{Gazit2009}
\citation{WTM2012a}
\citation{SU2018}
\citation{BK2008}
\citation{BK2008}
\citation{Belta2017}
\citation{Belta2017}
\citation{Puterman}
\citation{WTM2012}
\citation{Sadigh2014}
\citation{Sutton}
\citation{Sadigh2014}
\citation{HU2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\citation{SEJK2016}
\citation{HAK2019}
\citation{Hahn2019}
\citation{HKAKPL2019}
\citation{BWZP2019}
\citation{Hahn2019}
\citation{BWZP2019}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{HAK2019}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov Decision Processes}{3}}
\newlabel{MDP}{{2.1}{3}}
\citation{ESS}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Reinforcement Learning}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Objective functions and an Optimal policy}{4}}
\newlabel{V_pi}{{2.1}{5}}
\newlabel{Q_pi}{{2.2}{5}}
\newlabel{opt_V}{{2.3}{6}}
\newlabel{opt_Q}{{2.4}{6}}
\newlabel{opt_pol}{{2.5}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Temporal Difference Learning}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Linear Temporal Logic and Automata}{8}}
\citation{HKAKPL2019}
\citation{SEJK2016}
\citation{SEJK2016}
\newlabel{def5}{{2.6}{9}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Reinforcement learning based control policy synthesis for LTL specifications}{10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Augmentation of tLDGBAs and Synthesis Method}{10}}
\newlabel{augment_def}{{3.1}{11}}
\newlabel{prop3-1}{{3.1}{11}}
\newlabel{ltl}{{3.1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The tLDGBA recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $x_0$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to.}}{11}}
\newlabel{automaton}{{3.1}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The augmented automaton for the tLDGBA in Fig.\nobreakspace  {}3.1\@setref@ {} recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $(x_0, (0,0)^T )$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to. All states corresponding to $x_1$ are merged into $(x_1, (*,*)^T )$.}}{12}}
\newlabel{automaton_aug}{{3.2}{12}}
\newlabel{def9}{{3.2}{12}}
\newlabel{def10}{{3.3}{12}}
\newlabel{lemma3-1}{{3.1}{13}}
\newlabel{theorem3-1}{{3.1}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Example}{13}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces RL-based synthesis of control policy on the MDP with the augmented tLDBA.}}{14}}
\newlabel{syn_pol}{{1}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The environment consisting of eight rooms and one corridor. Red arcs are the transitions that we want to occur infinitely often, while blue arcs are the transitions that we never want to occur. $s_7$ is the initial state.}}{14}}
\newlabel{Grid1}{{3.3}{14}}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{Singh1998}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{HAK2019}
\citation{HKAKPL2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The mean of average reward in each episode for 20 learning sessions obtained from our proposed method (left) and the method using tLDBA (right). They are plotted per 50 episodes and the green areas represent the range of standard deviations. }}{15}}
\newlabel{result}{{3.4}{15}}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{HAK2019}
\citation{HKAKPL2019}
\citation{HAK2019}
\citation{HKAKPL2019}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The optimal policy obtained from our proposed method (left) and the method in \cite  {HAK2019, HKAKPL2019} (right).}}{16}}
\newlabel{optimal}{{3.5}{16}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Reinforcement learning based supervisor synthesis for LTL specifications}{17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Stochastic Discrete Event Systems}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Product DESs}{19}}
\newlabel{reward_def}{{4.6}{20}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Learning Algorithm}{20}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces $P_E$ inference.}}{21}}
\newlabel{bayes}{{2}{21}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces RL-based synthesis of a supervisor satisfying the given LTL specification.}}{22}}
\newlabel{alg1}{{3}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Example}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces The maze of the cat and the mouse. the initial state of the cat and the mouse is $s_0$ and $s_2$, respectively. the food for them is in the room 1 ($s_1$).}}{24}}
\newlabel{cat_mouse}{{4.1}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The estimated optimal state values at the initial state $V(s^{\otimes }_{init})$ with $r_{n} = 0.1$ (left above), $r_n = 0.7$ (right above), and $r_n = 1.2$ (below) when using Algorithm 3\@setref@ {}.}}{24}}
\newlabel{result1}{{4.2}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The average rewards for $\mathcal  {R}_1$ and average rewards for $\mathcal  {R}_2$ by the supervisor obtained from the learning with $r_{n} = 0.1$ (left above), $r_n = 0.7$ (right above), and $r_n = 1.2$ (below).}}{25}}
\newlabel{sim1}{{4.3}{25}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusions}{26}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Proofs}{27}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{eq:inf_r}{{A.1}{27}}
\citation{ESS}
\citation{ESS}
\newlabel{eq15}{{A.2}{28}}
\citation{ESS}
\citation{ISP}
\citation{ESS}
\citation{SM}
\@writefile{toc}{\contentsline {chapter}{Acknowledgment}{31}}
\bibcite{BK2008}{1}
\bibcite{Clarke2018}{2}
\bibcite{KB2008}{3}
\bibcite{Gazit2009}{4}
\bibcite{WTM2012a}{5}
\bibcite{SU2018}{6}
\bibcite{Belta2017}{7}
\bibcite{Puterman}{8}
\bibcite{WTM2012}{9}
\bibcite{Sadigh2014}{10}
\bibcite{Sutton}{11}
\bibcite{HU2015}{12}
\bibcite{SEJK2016}{13}
\@writefile{toc}{\contentsline {chapter}{References}{32}}
\bibcite{HAK2019}{14}
\bibcite{Hahn2019}{15}
\bibcite{HKAKPL2019}{16}
\bibcite{BWZP2019}{17}
\bibcite{ESS}{18}
\bibcite{ISP}{19}
\bibcite{SM}{20}
\bibcite{Singh1998}{21}
\bibcite{Owl}{22}
