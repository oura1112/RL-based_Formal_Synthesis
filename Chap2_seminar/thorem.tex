\documentclass[10pt]{article}

\usepackage{amsfonts,amsmath,amssymb}
\usepackage{bm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{color}
%\usepackage[dvipdfmx]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage{txfonts}
%\usepackage{ascmac, here}
\usepackage{listings}
\usepackage{color}
%\usepackage{url}
\usepackage{comment}

\allowdisplaybreaks[1]

\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{proof}{Proof}
\newtheorem{prop}{Proposition}
\newtheorem{definition}{Definition}

%\theoremstyle{definition}
%\newtheorem{definition}{Definition}[section]

%\theoremstyle{remark}
%\newtheorem*{remark}{Remark}

\newcommand{\mysps}{\ensuremath{[\![s^{\otimes}]\!]}_s}
\newcommand{\myspq}{\ensuremath{[\![s^{\otimes}]\!]}_q}
\newcommand{\myspds}{\ensuremath{[\![s^{\otimes \prime}]\!]}_s}
\newcommand{\myspdq}{\ensuremath{[\![s^{\otimes \prime}]\!]}_q}
\newcommand{\argmax}{\mathop{\rm arg~max}\limits}
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}

\begin{document}

\begin{lemma}
  For any policy $\pi$ and any recurrent class $R^{\otimes i}_{\pi}$ in the Markov chain $MC^{\otimes}_{\pi}$,
  $MC^{\otimes}_{\pi}$ satisfies one of the following conditions.
  \vspace{2mm}
  \begin{enumerate}
    \item $\delta^{\otimes}_{\pi,i} \cap \bar{F}^{\otimes}_j \neq \emptyset\ $, $ \forall j \in \{ 1, \ldots ,n \}$,
    \item $\delta^{\otimes}_{\pi,i} \cap \bar{F}^{\otimes}_j = \emptyset\ $, $ \forall j \in \{ 1, \ldots ,n \}$.
  \end{enumerate}
  \label{lemma3-1}
\end{lemma}

Let $\mathcal{SV}^{\ast}$ be the set of optimal supervisors. Let $D^{\otimes}_{SV^{\ast}}$ denote the product DES $D^{\otimes}$ controlled by the optimal supervisor $SV^{\ast}$.

For a Markov chain $MC^{\otimes}_{SV}$ induced by a product MDP $D^{\otimes}$ with a supervisor $SV$, let $S^{\otimes}_{SV}= T^{\otimes}_{SV} \sqcup R^{\otimes 1}_{SV} \sqcup \ldots \sqcup R^{\otimes h}_{SV}$ be the set of states in $MC^{\otimes}_{SV}$, where $T^{\otimes}_{SV}$ is the set of transient states and $R^{\otimes i}_{SV}$ is the recurrent class for each $i \in \{ 1, \ldots ,h \}$, and let $R(MC^{\otimes}_{SV})$ be the union of all recurrent classes in $MC^{\otimes}_{SV}$. Let $\delta^{\otimes}_{SV,i}$ be the set of transtions in a recurrent class $R^{\otimes i}_{SV}$, namely $\delta^{\otimes}_{SV,i} = \{ (s^{\otimes}, e, s^{\otimes \prime}) \in \delta^{\otimes} ; s^{\otimes} \in R^{\otimes i}_{SV},\ P^{\otimes}_T(s^{\otimes \prime}|s^{\otimes}, e) > 0, P^{\otimes}_E(e | s^{\otimes}, SV(s^{\otimes})) > 0 \}$, and let $P^{\otimes}_{SV}$ : $S^{\otimes}_{SV} \times S^{\otimes}_{SV} \rightarrow [0,1]$ such that $P^{\otimes}_{SV} = \sum_{e \in SV(s^{\otimes})} P^{\otimes}_T (s^{\otimes \prime} | s^{\otimes}, e) P^{\otimes}_E (e | s^{\otimes}, SV(s^{\otimes}))$ be the transition probability under $SV$.

\begin{theorem}
  Let $D^{\otimes}$ be the product DES corresponding to a DES $D$ and an LTL formuula $\varphi$. If there exists a supervisor $SV$ satisfying $\varphi$, then there exist a discount factor $\gamma^{\ast}$ and a positive reward $r_p$ such that any algorithm that maximizes the expected discounted reward with $\gamma > \gamma^{\ast}$ and $r_p > ||\mathcal{R}_1||_{\infty}$ will find a supervisor satisfying $\varphi$.
\end{theorem}

\begin{proof}
  Suppose that $SV^{\ast}$ be an optimal supervisor but does not satisfy the LTL formula $\varphi$. Then, for any recurrent class $R^{\otimes i}_{{SV}^{\ast}}$ in the Markov chain $MC^{\otimes}_{{SV}^{\ast}}$ and any accepting set $\bar{F}^{\otimes}_j$ of the product DES $D^{\otimes}$,  $\delta^{\otimes}_{SV^{\ast},i} \cap \bar{F}^{\otimes}_j = \emptyset$
  holds by Lemma \ref{lemma3-1}. Thus, the agent under the policy $\pi^{\ast}$ can obtain rewards only in the set of transient states. We consider the best scenario in the assumption. Let $p^k(s,s^{\prime})$ be the probability of going to a state $s^{\prime}$ in $k$ time steps after leaving the state $s$, and let $Post(T^{\otimes}_{\pi^{\ast}})$ be the set of states in recurrent classes that can be transitioned from states in $T^{\otimes}_{\pi^{\ast}}$ by one event occurrence. For the initial state $s^{\otimes}_{init}$ in the set of transient states, it holds that

  \begin{align}
    V^{SV^{\ast}}\!(s^{\otimes}_{init})
     =\ & \sum_{k=0}^{\infty} \sum_{s^{\otimes} \in T^{\otimes}_{\pi^{\ast}}} \gamma^k p^k(s^{\otimes}_{init}, s^{\otimes}) \nonumber \\
     & \sum_{s^{\otimes \prime} \in T^{\otimes}_{\pi^{\ast}} \cup Post(T^{\otimes}_{\pi^{\ast}})}  \sum_{e \in SV(s^{\otimes})} P^{\otimes}_T (s^{\otimes \prime} | s^{\otimes}, e) P^{\otimes}_E (e | s^{\otimes}, SV(s^{\otimes})) \mathcal{R}(s^{\otimes}, SV(s^{\otimes}), e, s^{\otimes \prime})\nonumber \\
     \leq\ & r_p \sum_{k=0}^{\infty} \sum_{s^{\otimes} \in T^{\otimes}_{\pi^{\ast}}} \gamma^k p^k(s^{\otimes}_{init}, s^{\otimes}). \nonumber
  \label{eqth11}
  \end{align}
  By the property of the transient states, for any state $s^{\otimes}$ in $T^{\otimes}_{\pi^{\ast}}$, there exists a bounded positive value $m$ such that $ \sum_{k=0}^{\infty} \gamma^k p^k(s^{\otimes}_{init}, s^{\otimes}) \leq \sum_{k=0}^{\infty} p^k(s^{\otimes}_{init}, s^{\otimes}) < m$ \cite{ESS}. Therefore, there exists a bounded positive value $\bar{m}$ such that $V^{\pi^{\ast}}(s^{\otimes}_{init}) < \bar{m}$.
  Let $\bar{SV}$ be a supervisor satisfying $\varphi$. We consider the following two cases.

  \begin{enumerate}
    \vspace{2mm}
    \item Assume that the initial state $s^{\otimes}_{init}$ is in a recurrent class $R^{\otimes i}_{\bar{\pi}}$ for some $ i \in \{ 1,\ldots,h \} $.
    For any accepting set $\bar{F}^{\otimes}_j$, $\delta^{\otimes}_{\bar{\pi},i} \cap \bar{F}^{\otimes}_j \neq \emptyset$ holds by the definition of $\bar{\pi}$. The expected discounted reward for $s^{\otimes}_{init}$ is given by
    %\begin{comment}
    %\begin{align}
    %  V^{\bar{\pi}}(s^{\otimes}_{init})
    %   &= \sum_{k=0}^{\infty} \sum_{s^{\otimes} \in R^{\otimes i}_{\bar{SV}}} \gamma^k p^k(s^{\otimes}_{init}, s^{\otimes}) \nonumber \\
    %   & \sum_{s^{\otimes \prime} \in R^{\otimes i}_{\bar{SV}}}  \sum_{e \in SV(s^{\otimes})} P^{\otimes}_T (s^{\otimes \prime} | s^{\otimes}, e) P^{\otimes}_E (e | s^{\otimes}, SV(s^{\otimes})) \mathcal{R}(s^{\otimes}, \bar{SV}(s^{\otimes}), e, s^{\otimes \prime}). \nonumber
    %\end{align}
    %\end{comment}

  \begin{align}
    V^{\bar{SV}}(s^{\otimes}_{init}) = \mathbb{E}^{SV}[ {\sum_{k=0}^{\infty}} \gamma^k \mathcal{R}(s_k, \pi_k, e_k, s_{k+1}) | s_0 = s^{\otimes}_{init} ]
  \end{align}
    Since $s^{\otimes}_{init}$ is in $R^{\otimes i}_{\bar{\pi}}$, there exists a set of positive numbers $K = \{ k\ ;\ k \geq n, p^{k}(s^{\otimes}_{init}, s^{\otimes}_{init}) > 0 \}$ \cite{ESS}. We consider the worst scenario of returning the initial state in this case. For the stopping time $k$ of first returning to the initial state, it holds that

    \begin{align}
      V^{\bar{\pi}}(s^{\otimes}_{init})
       > & \mathbb{E}^{\bar{SV}} [ \gamma^k r_p - (1 + \ldots + \gamma^k) ||\mathcal{R}_1||_{\infty} + \gamma^k V^{\bar{\pi}}(s^{\otimes}_{init}) | s_0 = s^{\otimes}_{init} ] \nonumber \\
       \geq & \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} r_p - (1 + \ldots + \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} ) ||\mathcal{R}_1||_{\infty} + \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} V^{\bar{\pi}}(s^{\otimes}_{init}) \nonumber \\
       = & \frac{ \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} r_p - (1 + \ldots + \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} ) ||\mathcal{R}_1||_{\infty} } { 1 - \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]}}, \nonumber
    \end{align}
 where second inequality holds since it holds that $\mathbb{E}^{\bar{SV}} [ \gamma^k | s_0 = s^{\otimes}_{init} ] \geq \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]}$ and $ \frac{1 - \gamma^{\mathbb{E}^{\bar{SV}} [ {k+1} | s_0 = s^{\otimes}_{init} ]}}{1 - \gamma} \leq \mathbb{E}^{\bar{SV}} [ \frac{1 - \gamma^{k+1}}{1 - \gamma} | s_0 = s^{\otimes}_{init} ]$ by Jensen's inequality.
  Therefore, for any $\bar{m} \in (V^{SV^{\ast}}(s^{\otimes}_{init}), \infty)$ and any reward function $\mathcal{R}_1$, there exist $\gamma^{\ast} < 1$ and a positive reward $r_p$ such that $\gamma > \gamma^{\ast} $ and $\gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} r_p - (1 + \ldots + \gamma^{\mathbb{E}^{\bar{SV}}[k | s_0 = s^{\otimes}_{init} ]} ) ||\mathcal{R}_1||_{\infty}$ imply $V^{\bar{SV}}(s^{\otimes}_{init}) > \bar{m} > V^{SV^{\ast}}(s^{\otimes}_{init})$.

  \item Assume that the initial state $s^{\otimes}_{init}$ is in the set of transient states $T_{\bar{SV}}^{\otimes}$.$P^{M^{\otimes}}_{\bar{SV}}(s^{\otimes}_{init} \models \varphi) > 0$ holds by the definition of $\bar{SV}$. For a recurrent class $R^{\otimes i}_{\bar{SV}}$ such that $\delta^{\otimes}_{\bar{SV}, i} \cap \bar{F}^{\otimes}_j \neq \emptyset$ for each accepting set
    $\bar{F}^{\otimes}_j$, there exist a number $\bar{l} > 0$, a state $\hat{s}^{\otimes}$ in $Post(T^{\otimes}_{\bar{SV}}) \cap R^{\otimes i}_{\bar{SV}}$, and a subset of transient states $\{ s^{\otimes}_1, \ldots , s^{\otimes}_{\bar{l}-1} \} \subset T^\otimes_{\bar{SV}}$ such that $p(s^{\otimes}_{init}, s^{\otimes}_1)>0$, $p(s^{\otimes}_{i}, s^{\otimes}_{i+1})>0$ for $i \in \{ 1,...,\bar{l}-2 \}$, and $p(s^{\otimes}_{\bar{l}-1}, \hat{s}^{\otimes})>0$ by the property of transient states.
    Hence, it holds that $p^{\bar{l}}(s^{\otimes}_{init}, \hat{s}^{\otimes}) > 0$ for the state $\hat{s}^{\otimes}$. Thus, for the stopping time $k$ of first returning to the state $\hat{s}^{\otimes}$, by ignoring positive rewards in $T^{\otimes}_{\bar{\pi}}$ and assuming the system incurs the full costs regarding events prohibition, we have

    \begin{align}
      V^{\bar{\pi}}(s^{\otimes}_{init})
        \geq\ & P^{M^{\otimes}}_{\bar{\pi}}(s^{\otimes}_{init} \models \varphi)  \gamma^{\bar{l}} p^{\bar{l}}(s^{\otimes}_{init}, \hat{s}^{\otimes}) \nonumber \\
        & \mathbb{E}^{\bar{SV}} [ \gamma^k r_p - (1 + \ldots + \gamma^k) ||\mathcal{R}_1||_{\infty} + \gamma^k V^{\bar{\pi}}(\hat{s}^{\otimes}) | s_{\hat{l}} = \hat{s}^{\otimes} ] \nonumber \\
        \geq & P^{M^{\otimes}}_{\bar{\pi}}(s^{\otimes}_{init} \models \varphi)  \gamma^{\bar{l}} p^{\bar{l}}(s^{\otimes}_{init}, \hat{s}^{\otimes}) \nonumber \\
        & \frac{ \gamma^{\mathbb{E}^{\bar{SV}}[k | s_{\hat{l}} = \hat{s}^{\otimes} ]} r_p - (1 + \ldots + \gamma^{\mathbb{E}^{\bar{SV}}[k | s_{\hat{l}} = \hat{s}^{\otimes} ) ||\mathcal{R}_1||_{\infty} } { 1 - \gamma^{\mathbb{E}^{\bar{SV}}[k | s_{\hat{l}} = \hat{s}^{\otimes} ]}}, \nonumber
    \end{align}

  \end{enumerate}
\end{proof}

\begin{thebibliography}{99}
\bibitem{ESS}
R.\ Durrett,
\textit{Essentials of Stochastic Processes}, 2nd Edition. ser. Springer texts in statistics. New York; London; Springer, 2012.
\bibitem{ISP}
L.\ Breuer,
``Introduction to Stochastic Processes,'' [Online]. Available: https://www.kent.ac.uk/smsas/personal/lb209/files/sp07.pdf
\bibitem{SM}
S.M.\ Ross,
\textit{Stochastic Processes}, 2nd Edition. University of California, Wiley, 1995.
\bibitem{Singh1998}
S. Singh, T. Jaakkola, M. L. Littman, and C. Szepes\'{v}ari,
``Convergence results for single-step on-policy reinforcement learning algorithms'' \textit{Machine Learning},
vol.~38, no.~3, pp,~287--308, 1998.
\bibitem{Owl}
J.~Kretínsk\'{y}, T.~Meggendorfer, S.~Sickert, ``Owl: A library for $\omega$-words, automata,
and LTL,'' in \textit{Proc.~16th International Symposium on Automated Technology for Verification and Analysis}, 2018,  pp.~543–550.
%https://doi.org/10.1007/978-3-030-01090-4\_34

\end{thebibliography}
\end{document}
