\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}System Model}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Objective function for Control patterns}{2}\protected@file@percent }
\newlabel{reward_def}{{2.2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning Algorithm}{3}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $P_E$ inference.}}{4}\protected@file@percent }
\newlabel{bayes}{{1}{4}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces RL-based synthesis of a supervisor satisfying the given LTL specification.}}{5}\protected@file@percent }
\newlabel{alg1}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Example}{5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The maze of the cat and the mouse. the initial state of the cat and the mouse is $s_0$ and $s_2$, respectively. the food for them is in the room 1 ($s_1$).}}{7}\protected@file@percent }
\newlabel{cat_mouse}{{1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The tLDGBA recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $x_0$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to, e.g., \textcircled {\relax \fontsize  {7}{8}\selectfont  1}$a \land \neg b \land \neg c$ means the transition labeled by it belongs to the accepting set $F_1$.}}{7}\protected@file@percent }
\newlabel{tldba}{{2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The estimated optimal state values at the initial state $V(s^{\otimes }_{init})$ with $r_{n} = 0.1$ (left above), $r_n = 0.7$ (right above), and $r_n = 1.2$ (below) when using Algorithm 2\@setref@ {}.}}{8}\protected@file@percent }
\newlabel{result1}{{3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The average rewards of $\mathcal  {R}_1$ and average rewards of $\mathcal  {R}_2$ by the supervisor obtained from the learning with $r_{n} = 0.1$ (left above), $r_n = 0.7$ (right above), and $r_n = 1.2$ (below).}}{9}\protected@file@percent }
\newlabel{sim1}{{4}{9}}
