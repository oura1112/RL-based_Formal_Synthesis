\relax 
\@writefile{toc}{\contentsline {section}{\numberline {I}System Model}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Objective function for Control patterns}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Learning Algorithm}{2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces $P_E$ inference.}}{2}\protected@file@percent }
\newlabel{bayes}{{1}{2}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces RL-based synthesis of a supervisor satisfying the given LTL specification.}}{2}\protected@file@percent }
\newlabel{alg1}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Example}{2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The maze of the cat and the mouse. the initial state of the cat and the mouse is $s_0$ and $s_2$, respectively. the food for them is in the room 1 ($s_1$).}}{3}\protected@file@percent }
\newlabel{cat_mouse}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The tLDBA recognizing the LTL formula $\text  {{\bf  GF}}a \wedge \text  {{\bf  GF}}b \wedge \text  {{\bf  G}}\neg c$, where the initial state is $x_0$. Red arcs are accepting transitions that are numbered in accordance with the accepting sets they belong to, e.g., \textcircled {\relax \fontsize  {7}{8pt}\selectfont  1}$a \land \neg b \land \neg c$ means the transition labeled by it belongs to the accepting set $F_1$.}}{3}\protected@file@percent }
\newlabel{tldba}{{2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces the estimated optimal state value function at the initial state $V(s^{\otimes }_{init})$ with $r_{n1} = -0.1$ when using the algorithm 2\@setref@ {}.}}{3}\protected@file@percent }
\newlabel{result1}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces the estimated optimal state value function at the initial state $V(s^{\otimes }_{init})$ with $r_{n1} = -0.5$ when using the algorithm 2\@setref@ {}.}}{3}\protected@file@percent }
\newlabel{result2}{{4}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces the estimated optimal state value function at the initial state $V(s^{\otimes }_{init})$ with $r_{n1} = -1$ when using the algorithm 2\@setref@ {}.}}{4}\protected@file@percent }
\newlabel{result3}{{5}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The average reward and average cost by the supervisor obtained from the learning with $r_{n1} = -0.1$.}}{4}\protected@file@percent }
\newlabel{sim1}{{6}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The average reward and average cost by the supervisor obtained from the learning with $r_{n1} = -0.5$.}}{4}\protected@file@percent }
\newlabel{sim2}{{7}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The average reward and average cost by the supervisor obtained from the learning with $r_{n1} = -1$.}}{4}\protected@file@percent }
\newlabel{sim3}{{8}{4}}
